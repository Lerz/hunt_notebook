# üìò Documentation ‚Äì Utilisation d‚Äôun LLM local avec JupyterLab (Airgapped)

Ce guide explique comment utiliser efficacement un **LLM 3B** (Gemma, Mistral‚Ä¶) en local avec `llama-cpp-python` dans un environnement de hunt offline sous **JupyterLab**.

---

## üéØ Objectif

- Interroger un mod√®le local (format `.gguf`) depuis Jupyter
- Ne charger le mod√®le **qu‚Äôune seule fois** pour optimiser les performances
- Faciliter l‚Äôanalyse automatique des logs r√©seau enrichis

---

## üìÅ Organisation des fichiers

```
/models/
    gemma-2b-it.Q4_K_M.gguf
/datasets/
    stormshield_logs.csv
/output/
    stormshield_enriched.csv
/llm_wrapper.py
/prompt_utils.py
/llm_hunt_notebook.ipynb
```

---

## ‚öôÔ∏è Pr√©requis

Librairies Python :

```
llama-cpp-python
pandas
jupyterlab
```

Installe-les via `pip` :

```bash
pip install llama-cpp-python pandas jupyterlab
```

---

## üß† Chargement du mod√®le

Dans une cellule Jupyter :

```python
from llm_wrapper import HuntLLM

# Cr√©e une instance globale du mod√®le (charg√© une seule fois en RAM)
llm = HuntLLM(model_path="./models/gemma-2b-it.Q4_K_M.gguf", threads=8)
print("‚úÖ Mod√®le charg√©.")
```

---

## üìä Formater les logs pour le prompt

```python
import pandas as pd
from prompt_utils import format_logs_for_prompt

df = pd.read_csv("./output/stormshield_enriched.csv")
context = format_logs_for_prompt(df)
```

---

## üí¨ Interroger le mod√®le

```python
question = "Quels signes d‚Äôexfiltration observes-tu dans ces logs r√©seau ?"
response = llm.ask(question, context=context, max_tokens=300)
print(response)
```

Tu peux appeler `llm.ask(...)` autant de fois que n√©cessaire **sans relancer l‚Äôinitialisation**.

---

## üí° Exemples de questions utiles

```text
- D√©cris les comportements r√©seau suspects dans cet extrait
- Y a-t-il du DNS tunneling ?
- Quelles IP semblent suspectes ?
- Quels domaines sont potentiellement des dropzones ?
- Propose une r√®gle SPL de d√©tection
```

---

## üß† Boucle interactive (optionnelle)

```python
while True:
    q = input("Ta question > ")
    print(llm.ask(q, context=context))
```

---

## ‚úÖ R√©sum√©

| √âtape                   | Action                                        |
|-------------------------|-----------------------------------------------|
| Charger le mod√®le       | `llm = HuntLLM(...)`                          |
| Cr√©er un contexte       | `context = format_logs_for_prompt(df)`       |
| Poser une question      | `llm.ask("...", context=context)`            |
| R√©utilisable ?          | Oui, tant que le kernel n‚Äôest pas red√©marr√© |

---

